<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://seanbecker.me</id>
    <title>Sean Becker | Feed</title>
    <updated>2026-02-08T23:31:15.165Z</updated>
    <generator>Feed for Node.js</generator>
    <link rel="alternate" href="https://seanbecker.me"/>
    <link rel="self" href="https://seanbecker.me/atom.xml"/>
    <subtitle>Technology and Engineering Blog</subtitle>
    <logo>https://seanbecker.me/logo.png</logo>
    <icon>https://seanbecker.me/favicon.ico</icon>
    <rights>Copyright 2026 Sean Becker</rights>
    <entry>
        <title type="html"><![CDATA[About Me]]></title>
        <id>https://seanbecker.me/blog/about-me</id>
        <link href="https://seanbecker.me/blog/about-me"/>
        <updated>2023-12-03T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[Just a few snippets of information to get to know me.]]></summary>
        <content type="html"><![CDATA[
- I grew up in Vernon Hills, IL
- I went to Purdue University where I majored in Computer Science and minored in Business Management
- I try to take at least one ski trip every year, usually on the west coast of the US
- I frequently listen to podcasts including Lex Fridman, Andrew Huberman, Joe Rogan, and Sam Harris
- I enjoy building all things related to technology. Please reach out if you have an idea and want advice on how to build it :)

Go back [home](/).
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Past Projects]]></title>
        <id>https://seanbecker.me/blog/past-projects</id>
        <link href="https://seanbecker.me/blog/past-projects"/>
        <updated>2023-12-03T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[This is a list of some of the projects I've worked on in the past.]]></summary>
        <content type="html"><![CDATA[
<ProjectList filter={({ active }) => !active} />

Go back [home](/).
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenAI Series: Chat-GPT Style Completion Using SSE Streaming API]]></title>
        <id>https://seanbecker.me/blog/chatgpt-text-completion</id>
        <link href="https://seanbecker.me/blog/chatgpt-text-completion"/>
        <updated>2023-12-04T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[Use the OpenAI SSE streaming API to make a chatbot that can complete sentences just like ChatGPT.]]></summary>
        <content type="html"><![CDATA[
Just looking for code? You can find that [here](https://gist.github.com/seanbecker15/49a20ef17e77682d7907f5eba8fd507b).

## Introduction

There are lots of things that make ChatGPT great under the hood, but let's face it: most people will never truly understand the internals.
That doesn't mean that the entire stack has to be a mystery!

### Why UX is important

If you've ever used the OpenAI API you'll know that it's quite slow to use the standard API without streaming. It can take 10+ seconds to a receive
a response depending on the complexity of the prompt. Although it is [probably a myth](https://www.reddit.com/r/webdev/comments/n66d0r/comment/gx5b2gl) that users will leave your website if it takes more than 3 seconds to load,
it is still super important to have good _perceived performance_. For companies working on cutting edge innovation such as OpenAI it is even more important to prove that their tool is useful.
Waiting for 10+ seconds leads to loss of context and a bad user experience.

Though the total response time is likely the same, the streaming API sends back partial responses as they are generated.
Users can see the response in real time and process the response as it is being generated. This makes a huge difference in the user experience.

### Assumptions

- I'm using React for this example, but most of this code should be very easy to plug into other frameworks.
- I'm assuming that you already have a backend endpoint that forwards the OpenAI stream to your frontend. If not, I will be making a guide on how to do that soon.

### Prerequisites

```bash
npm install eventsource-parser
```

## Let's get coding

1. Scaffold our component. We'll need a prompt input, a submit button, and a location for our results.

```javascript
import { useState, ChangeEvent } from "react";

// Assumes that you can already fetch a completion from OpenAI
import { fetchCompletion } from "./api";

const Chatbot = () => {
  const [prompt, setPrompt] = useState("");
  const [completion, setCompletion] = useState("");

  const handleResponse = async (response: Response) => {};

  const handleChange = (e: ChangeEvent<HTMLInputElement>) => {
    setPrompt(e.target.value);
  };

  const handleSubmit = () => fetchCompletion(prompt)
    .then(handleResponse)

  return (
    <div>
      <div>
        <input type="text" value={prompt} onChange={handleChange} />
        <button type="button" onClick={handleSubmit}>Submit</button>
      </div>
      <p>{completion}</p>
    </div>
  );
}
```

2. Convert OpenAI stream to ReadableStream. OpenAI sends a stream of bytes. We need to convert that to a stream of strings.

```javascript
...

const Chatbot = () => {
  ...

  const handleResponse = async (response: Response) => {
    const stream = response.body;
    const textStream = stream.pipeThrough(new TextDecoderStream());
  }

  ...
}
```

3. Make ReadableStream async iterable. ReadableStream is not yet an async iterable in browsers, so we need to convert it to one.
   If our target platform were NodeJS we wouldn't need this step as ReadableStream is already an async iterable in NodeJS.

```javascript
  ...

  // Convert the stream to an async iterator.
  // Found here https://jakearchibald.com/2017/async-iterators-and-generators/#making-streams-iterate
  async function* streamAsyncIterator(stream: ReadableStream) {
    const reader = stream.getReader();
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) return;
        yield value;
      }
    } finally {
      reader.releaseLock();
    }
  }

  ...

  const Chatbot = () => {
    ...

    const handleResponse = async (response: Response) => {
      const stream = response.body;
      const textStream = stream.pipeThrough(new TextDecoderStream());
      const asyncIteratorStream = streamAsyncIterator(textStream);
      for await (const chunk of asyncIteratorStream) {
        console.log(chunk);
      }
    }

    ...
  }
```

4. Previously we were logging each chunk to the console. Now we need to parse each chunk as an event. We'll use the `eventsource-parser` package to do this.

```javascript
...

import { createParser, ParsedEvent, ReconnectInterval } from "eventsource-parser";

...

const Chatbot = () => {
  ...

  const parseEvent = (event: ParsedEvent | ReconnectInterval) => {
    console.log(event);
  };

  const handleResponse = async (response: Response) => {
    ...
    const parser = createParser(parseEvent);
    for await (const chunk of asyncIteratorStream) {
      parser.feed(chunk);
    }
  }

  ...
}

```

5. Update completion text as we get each event. You should now see the completion text update as it is generated.

```javascript
...

const Chatbot = () => {
  ...

  const parseEvent = (event: ParsedEvent | ReconnectInterval) => {
    if (event.type !== 'event' || event.data === '[DONE]') {
      return
    }

    const delta = JSON.parse(event.data).choices[0]?.delta?.content || "";
    setCompletion((prev) => {
      if (prev) {
        return prev + delta;
      } else {
        return delta;
      }
    });
  };

  ...
}
```

## Recap

That's it! You should now have a working chatbot that can complete sentences in real time. You can find the full code [here](https://gist.github.com/seanbecker15/49a20ef17e77682d7907f5eba8fd507b).
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LG TV Wake on LAN]]></title>
        <id>https://seanbecker.me/blog/lg-wake-on-lan</id>
        <link href="https://seanbecker.me/blog/lg-wake-on-lan"/>
        <updated>2024-06-21T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[NodeJS script to wake LG (WebOS) TV over LAN]]></summary>
        <content type="html"><![CDATA[
Just looking for code? You can find that [here](https://gist.github.com/seanbecker15/456ddec169eb68aa7e48d38234bb5283).

## Background

Part of the challenge while working at Fubo is that we maintain several SmartTV applications.
In order to reduce the overhead of maintenance we rely on tooling, one of which is the [ares-cli](https://webostv.developer.lge.com/develop/tools/cli-dev-guide).
Unfortunately this CLI doesn't expose functionality to wake a TV up over the network, which is problematic for test automation.
It also means that I need to get up from my desk to turn on my TV before launching a debug version of our application.

## Solution

I found several sources that wake LG over the network such as

- [home assistant](https://github.com/home-assistant/core/blob/2ad5b1c3a6140a49d1113e86e46b68165cf26884/homeassistant/components/wake_on_lan/__init__.py#L34C15-L34C32)
- [LGTVCompanion](https://github.com/JPersson77/LGTVCompanion)

Both of these solutions are embedded in bigger systems so I decided take the code from [Console.cpp](https://github.com/JPersson77/LGTVCompanion/blob/5c63223a9866bc11965a3d9c9240313694c9c3f6/LGTV%20Companion%20Console/Console.cpp#L1531) and convert it to javascript.
This can be quickly ported to another language using a good LLM like ChatGPT or Claude.

You can find the javascript code [here](https://gist.github.com/seanbecker15/456ddec169eb68aa7e48d38234bb5283).
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Professional Experience]]></title>
        <id>https://seanbecker.me/blog/professional-experience</id>
        <link href="https://seanbecker.me/blog/professional-experience"/>
        <updated>2024-09-14T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[A brief summary of my professional experience.]]></summary>
        <content type="html"><![CDATA[
## FuboTV - Senior Software Engineer (2022 - Present)

- Migrated native video players to generic Media Source Extension (MSE) video players on Smart TV devices.
- Built ecosystem of tools to increase team efficiency:
  - Introduced datadog logging and instrumentation to enhance troubleshooting capabilities and gather feedback on releases.
  - Built tool to query logs and generate metrics using ChatGPT function calls to assist with gathering performance insights.
- [Contributed to redesigned application](https://cordcuttersnews.com/fubo-is-rolling-out-a-new-improved-user-interface-with-re-designed-apps/) (launched to 300,000+ users).

## Fubo Gaming - Software Engineer (2021 - 2022)

- Interviewed 70+ candidates for engineering roles. Scaled team from 5 to 15 engineers.
- Coordinated regulatory changes required to launch Fubo Sportsbook in Iowa, Arizona, and New Jersey.
- Rewrote authentication flow & integration between sportsbook and TV
  product; Coordinated effort between white-label provider, in-house
  designers, in-house frontend team, and in-house platform team.
- Integrated [Trustly](https://us.trustly.com) payment provider, simplifying deposit flow and substantially increasing first time deposit rate.

## Prior Employment

For information on prior employment, you can view my resume [here](/docs/resume.pdf).

Go back [home](/).
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Japan Trip Itinerary]]></title>
        <id>https://seanbecker.me/blog/japan-trip</id>
        <link href="https://seanbecker.me/blog/japan-trip"/>
        <updated>2025-04-20T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[May 10 – May 24, 2025]]></summary>
        <content type="html"><![CDATA[
## Flights

- **Joey**: Arrive May 10, 2:45 PM NRT | Depart May 24, 4:30 PM NRT
- **Sean**: Arrive May 10, 3:25 PM NRT | Depart May 24, 2:55 PM NRT

---

## Saturday, May 10 — Tokyo

**Booked**

- Stay: Far East Village Hotel Tokyo, Asakusa — [Google Maps: 1-11-6 Asakusa, Taito-ku, Tokyo](https://www.google.com/maps/search/?api=1&query=1-11-6+Asakusa,+Taito-ku,+Tokyo,+Japan,+111-0032)
- 5:00 PM – Arrive & check in
- 6:35 PM – Sunset at [Tokyo Skytree](https://www.google.com/maps/search/?api=1&query=Tokyo+Skytree)

_Optional_

- Imperial Palace East Gardens — [Google Maps](https://www.google.com/maps/search/?api=1&query=Imperial+Palace+East+Gardens)
- Tsukiji Outer Market — [Google Maps](https://www.google.com/maps/search/?api=1&query=Tsukiji+Outer+Market)

---

## Sunday, May 11 — Tokyo

**Booked**

- Stay: Far East Village Hotel Tokyo, Asakusa — [Maps link above]
- 10:00 AM – [Tokyo Free Walking Tour](https://www.tokyolocalized.com/free-walking-tour)
- 6:00 PM – Omakase in Ginza (reservation) — [Google Maps: Ginza](https://www.google.com/maps/search/?api=1&query=Ginza,+Tokyo)

_Optional_

- [teamLab Borderless](https://borderless.teamlab.art)
- [Shinjuku Night Walking Tour](https://www.tokyolocalized.com/tokyo-night-walking-tour-shinjuku-kabukicho)

---

## Monday, May 12 — Tokyo → Kyoto

**Booked**

- Morning – [Sumida River Cruise](https://www.suijobus.co.jp/en/)
- Late AM – [teamLab Planets](https://teamlabplanets.dmm.com/en/)
- Afternoon – Shinkansen to Kyoto
- Stay: Rinn Gion Kenninji — [Google Maps](https://www.google.com/maps/search/?api=1&query=Rinn+Gion+Kenninji,+Kyoto)

_Optional_

- [Fushimi Inari Taisha](https://inari.jp/en/)
- [Nijo Castle](https://www.city.kyoto.lg.jp/sankan/page/0000004002.html)
- Nishiki Market — [Google Maps](https://www.google.com/maps/search/?api=1&query=Nishiki+Market)

---

## Tuesday, May 13 — Kyoto

**Booked**

- Stay: Rinn Gion Kenninji — [Maps link above]
- Morning – [Arashiyama Bamboo Grove](https://www.google.com/maps/search/?api=1&query=Arashiyama+Bamboo+Grove)

_Optional_

- [Samurai Museum](https://www.samuraimuseum.jp/en/)
- Tea ceremony & kimono rental

---

## Wednesday, May 14 — Kyoto

**Booked**

- Stay: Rinn Gion Kenninji — [Maps link above]
- Morning – Kuya-no-taki Waterfall hike — [InsideKyoto guide](https://www.insidekyoto.com/takao-hozukyo-hike-via-kiyotaki-kuya-no-taki-waterfall)

_Optional_

- [Yamazaki Whisky Distillery](https://www.suntory.com/factory/yamazaki/)

---

## Thursday, May 15 — Kyoto

**Booked**

- Stay: Rinn Gion Kenninji — [Maps link above]
- All day – Wander the Gion area

_Optional_

- Fushimi Inari Taisha
- Nijo Castle
- Nishiki Market
- Samurai Museum

---

## Friday, May 16 — Hiroshima → Miyajima

**Booked**

- Morning – Shinkansen to Hiroshima & visit [Peace Memorial Museum](https://hpmmuseum.jp/?lang=en)
- 3:00 PM – Check in at Iwaso Ryokan — [Google Maps](https://www.google.com/maps/search/?api=1&query=Iwaso+Ryokan+Miyajima)
- Evening – Onsen & kaiseki dinner

_Optional_

- Hiroshima Castle — [Google Maps](https://www.google.com/maps/search/?api=1&query=Hiroshima+Castle)
- Itsukushima Shrine (Miyajima) — [Google Maps](https://www.google.com/maps/search/?api=1&query=Itsukushima+Shrine)

---

## Saturday, May 17 — Miyajima → Osaka

**Booked**

- Morning – Miyajima sightseeing (Itsukushima Shrine & Mt. Misen)
- Afternoon – Shinkansen to Osaka; check in at Agora Place Osaka Namba — [Google Maps](https://www.google.com/maps/search/?api=1&query=Agora+Place+Osaka+Namba)
- Evening – [Dōtonbori](https://www.google.com/maps/search/?api=1&query=Dotonbori+Osaka)

_Optional_

- Minayoshi sushi in Kadoma — [Google Maps](https://www.google.com/maps/search/?api=1&query=Minayoshi+Kadoma)
- Tenjinbashi-suji arcade — [Google Maps](https://www.google.com/maps/search/?api=1&query=Tenjinbashi-suji)
- Bunraku theatre — [Google Maps](https://www.google.com/maps/search/?api=1&query=Bunraku+Theatre+Osaka)

---

## Sunday, May 18 — Nara day-trip

**Booked**

- Morning – [Tōdai-ji & Deer Park](https://www.google.com/maps/search/?api=1&query=Todai-ji+Nara)
- Afternoon – [Kasuga Taisha](https://www.google.com/maps/search/?api=1&query=Kasuga+Taisha+Nara) & Nara-machi
- Evening – Return to Osaka (stay above)

_Optional_

- [Himeji Castle](https://www.google.com/maps/search/?api=1&query=Himeji+Castle)

---

## Monday, May 19 — Osaka

**Booked**

- Stay: Agora Place Osaka Namba — [Maps link above]

_Optional_

- Himeji Castle
- [Osaka Castle](https://www.google.com/maps/search/?api=1&query=Osaka+Castle)
- [Kuromon Market](https://www.google.com/maps/search/?api=1&query=Kuromon+Market)
- [Kōshien Stadium](https://www.google.com/maps/search/?api=1&query=Koshien+Stadium)

---

## Tuesday, May 20 — Osaka → Kobe dinner

**Booked**

- Kobe beef dinner — [Kobe Beef Association](https://www.kobe-niku.jp/en/top.html)

_Optional_

- [Kobe Harborland](https://www.google.com/maps/search/?api=1&query=Kobe+Harborland)
- [Mount Rokko night view](https://www.google.com/maps/search/?api=1&query=Mount+Rokko)

---

## Wednesday, May 21 — Osaka → Tokyo

**Booked**

- Travel to Tokyo; check in at Shinjuku Kuyakusho-mae Capsule Hotel — [Google Maps](https://www.google.com/maps/search/?api=1&query=Shinjuku+Kuyakusho-mae+Capsule+Hotel)
- 7:00 PM – [Shinjuku Night Walking Tour](https://www.tokyolocalized.com/tokyo-night-walking-tour-shinjuku-kabukicho)

_Optional_

- Golden Gai bar hop — [Google Maps](https://www.google.com/maps/search/?api=1&query=Golden+Gai+Shinjuku)
- VR arcade

---

## Thursday, May 22 — Tokyo

**Booked**

- Morning – [teamLab Borderless](https://borderless.teamlab.art)
- 6:00 PM – Yomiuri Giants game — [Google Maps: Tokyo Dome](https://www.google.com/maps/search/?api=1&query=Tokyo+Dome)

_Optional_

- [Meiji Shrine](https://www.google.com/maps/search/?api=1&query=Meiji+Shrine)
- [Takeshita Street, Harajuku](https://www.google.com/maps/search/?api=1&query=Takeshita+Street)
- Omotesando cafés

---

## Friday, May 23 — Tokyo

**Booked**

- Morning – [Sumida River Cruise](https://www.suijobus.co.jp/en/)
- 6:00 PM – Natsu Basho at Ryōgoku Kokugikan (tickets secured) — [Google Maps](https://www.google.com/maps/search/?api=1&query=Ryogoku+Kokugikan)

_Optional_

- Tsukiji Outer Market
- Whisky bar crawl (e.g. Bar High Five)

---

## Saturday, May 24 — Departure

**Booked**

- Flight home

_Optional_

- Last-minute shopping
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claiming My Own AT Protocol Handle (at://seanbecker.me)]]></title>
        <id>https://seanbecker.me/blog/claiming-my-own-at-handle</id>
        <link href="https://seanbecker.me/blog/claiming-my-own-at-handle"/>
        <updated>2025-10-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[How I connected my Bluesky account to my own domain using the AT Protocol.]]></summary>
        <content type="html"><![CDATA[
After reading [Dan Abramov’s post](https://overreacted.io/where-its-at/) about the AT Protocol, I became curious about how identity works on Bluesky. Using [atproto-browser.vercel.app](https://atproto-browser.vercel.app/), I realized how simple it is to connect your own domain to your account. My profile now lives at **at://seanbecker.me**.

## Steps

### 1. Find your DID

Go to [https://atproto-browser.vercel.app/](https://atproto-browser.vercel.app/) and look up your existing Bluesky handle, in my case `seanbex.bsky.social`.
My DID is:

```
did:plc:p2epzw3sltq7tdsg7sn7qenr
```

### 2. Add the `.well-known` verification file

On your website, create a file at:

```
https://seanbecker.me/.well-known/atproto-did
```

The file should contain only your DID:

```
did:plc:p2epzw3sltq7tdsg7sn7qenr
```

Make sure there are no extra spaces or newlines.

### 3. Update your handle in Bluesky

In the Bluesky app or on the web:

- Go to **Settings → Change handle**
- Select **I have my own domain**
- Enter `seanbecker.me`

Bluesky will check the `.well-known` file and update your handle once verified.

## Closing thoughts

The AT Protocol makes it possible to manage your online identity under your own domain, independent of any specific platform. It’s a small change that makes the web feel a little more open again.

Thanks to [Dan Abramov](https://overreacted.io) for the inspiration to explore this.
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Keeping a 20-Year Fantasy Football League Alive]]></title>
        <id>https://seanbecker.me/blog/keeping-fantasy-alive</id>
        <link href="https://seanbecker.me/blog/keeping-fantasy-alive"/>
        <updated>2025-12-15T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[How a small UI change broke a long-standing workflow and how automation fixed it without forcing anyone to change behavior.]]></summary>
        <content type="html"><![CDATA[
Source code found on [GitHub](https://github.com/seanbecker15/cbs-fantasy-tooling).

In 2020, CBS Sports changed their UI and broke a workflow my family had relied on for years.

Nothing dramatic happened at once. The site still worked. Picks still showed up. But a small detail changed under the hood. HTML tables were replaced with deeply nested divs, and copy and paste stopped behaving the way it always had.

What used to take about 5 to 10 minutes each week suddenly took close to an hour. Every Tuesday. For a fantasy football league that had already been running for nearly two decades, that friction mattered more than it sounds.

This repository exists for one reason: to keep a long-running league accurate, fair, and fun without forcing anyone to change how they participate.

No new accounts.  
No new platform.  
No retraining required.

---

## A League With History

Our league started in 2004. At first it was fully manual. Picks were written down and scores were tracked by hand. Eventually it moved to CBS Sports, but the league never really fit the default Pick’em mold.

Over time we added custom rules, weekly bonuses, and manual verification steps. Those rules became part of the league’s identity. They also made scoring more complicated than what CBS supported out of the box.

For years, that gap was handled with spreadsheets and process. It worked well enough until it didn’t.

---

## What Kind of League This Is

This is a confidence pool.

At a high level:

- Each week, participants pick the winner of every NFL game
- Each pick is assigned a unique confidence value
- Correct picks earn points equal to that confidence
- Incorrect picks earn zero

The strategy is not just about picking winners. It is about deciding where to place confidence relative to your other picks.

On top of that, we have a couple of extra rules that materially affect standings:

- **Most Wins Bonus**  
  The participant with the most correct picks in a week gets 5 bonus points

- **Most Points Bonus**  
  The participant with the highest total confidence points gets 10 bonus points  
  Ties are broken using the Monday Night Football total score

These bonuses are simple to explain but annoying to calculate, especially when you are doing it by hand every week.

---

## Why We Didn’t Switch Platforms

From a technical perspective, switching platforms or building something custom would have been easier.

From a practical perspective, it would have killed the league.

This is a group of family and family friends. Some people have been participating for decades. Asking everyone to create new accounts, learn a new UI, and change habits they have had for years would have created a lot of friction.

CBS Sports already hosts the league and it works well enough for picks. The goal was never to replace it. The goal was to work around its limitations.

CBS stays the system of record.

---

## The Spreadsheet Workflow

For a long time, my father handled scoring using an Excel spreadsheet with carefully built formulas. It supported:

- confidence scoring
- weekly bonuses
- tie breakers
- audits before results were finalized and emailed out

The weekly flow looked like this:

1. Copy results from CBS Sports
2. Paste them into Excel
3. Verify the math
4. Send out standings

From roughly 2010 through 2020, this took very little time. The process was stable and predictable.

After the CBS UI change, it wasn’t. Data pasted into the wrong columns. Rows would shift. Sometimes values were just missing. The same task now took close to an hour and required way more manual cleanup.

That was not sustainable.

---

## The First Fix

The first version of this project was intentionally narrow in scope.

I built a scraper that pulled league results directly from CBS Sports, normalized the data, and fed it back into the existing spreadsheet.

The spreadsheet stayed on purpose.

My father is an auditor by trade. Being able to review and spot check numbers before publishing results is non negotiable. He also genuinely enjoys that part of the process. The goal was not to replace him. It was to remove the brittle steps that had started breaking.

Automation supported the workflow instead of taking it over.

That scraper has now been running weekly for three seasons and has been boring in the best possible way.

---

## Why the Project Grew

Once reliable data ingestion existed, it was hard not to keep going.

With clean data available, new ideas became possible:

- combining historical league results with Vegas odds
- experimenting with different confidence allocation strategies
- building charts just to see what patterns show up
- creating APIs for live leaderboards and custom UIs

At that point, the project stopped being just a scraper. It turned into a place to experiment with automation and analysis around something we already cared about.

CBS Sports still stays the source of truth.

---

## What This Repository Does

At a high level, this project:

- ingests NFL and league data from multiple sources
- normalizes and analyzes that data
- feeds results into emails, charts, databases, and real time views

All without requiring league members to change platforms or behavior.

---

## What Comes Next

This post focused on why the project exists.

Future posts will dig into:

- how a broken manual workflow was automated without disrupting people
- how league history and Vegas odds can be combined to experiment with weekly picks
- how the same data pipeline powers charts, APIs, and live standings

Everything builds on the same idea: automate the fragile parts and keep humans in the loop where it matters.

---

## Thanks for Reading

This started as a small fix to keep a long running league operational and slowly grew into a place to experiment with data, automation, and analysis.

If you want to talk through any of the specifics, scoring rules, ingestion details, or design tradeoffs, you can reach me at:

- GitHub: https://github.com/seanbecker15
- X: https://x.com/theseanbecker
- Email: sean.becker15@gmail.com

More detailed posts will follow as each part of the system is explored in depth.
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rio Trip Itinerary]]></title>
        <id>https://seanbecker.me/blog/rio-trip</id>
        <link href="https://seanbecker.me/blog/rio-trip"/>
        <updated>2026-02-08T06:00:00.000Z</updated>
        <summary type="html"><![CDATA[Feb 10 – Feb 19, 2026]]></summary>
        <content type="html"><![CDATA[
## Flights

- **Sean**: Arrive Feb 10, 8:15 AM ORD -> ATL -> RIO | Depart Feb 19, 9:30 PM RIO -> ATL -> ORD

## Accommodation

- [Windsor Plaza Copacabana
  Ave Princesa Isabel 263 Copacabana, Rio de Janeiro, Brazil, 22011-010](https://www.google.com/maps/place/Windsor+Plaza+Copacabana/@-22.9627805,-43.1776329,17z/data=!4m9!3m8!1s0x997fff867cdf99:0x97e28a466acef850!5m2!4m1!1i2!8m2!3d-22.9627855!4d-43.175058!16s%2Fg%2F1tzl_5js?entry=ttu&g_ep=EgoyMDI2MDIwNC4wIKXMDSoASAFQAw%3D%3D)
- Check in @ 3pm on Tuesday, 2/10

## Other notes
- Erik's notion: [https://www.notion.so/Rio-Carnaval-2026-Homepage](https://www.notion.so/Rio-Carnaval-2026-Homepage-27b4964af572802cb854c206e450ed57)
- Guys' Airbnb: [Avenida Nossa Senhora de Copacabana, 198 201](https://www.google.com/maps/place/Av.+Nossa+Sra.+de+Copacabana,+198+-+201+-+Copacabana,+Rio+de+Janeiro+-+RJ,+22020-001,+Brazil/@-22.9651068,-43.179705,17z/data=!3m1!4b1!4m5!3m4!1s0x9bd5550a82deb5:0x1d699e4466dd1fd1!8m2!3d-22.9651118!4d-43.1771301?entry=ttu&g_ep=EgoyMDI2MDIwNC4wIKXMDSoASAFQAw%3D%3D)
- Gals' Airbnb: [Av. Atlântica, 2440 - Copacabana](google.com/maps/place/Av.+Atlântica,+2440+-+Copacabana,+Rio+de+Janeiro+-+RJ,+22041-001,+Brazil/@-22.9665498,-43.1809603,16.37z/data=!4m6!3m5!1s0x9bd54557ccde0b:0x8acaa63907e9ce75!8m2!3d-22.9713186!4d-43.1847121!16s%2Fg%2F11j8sdh6v2?entry=ttu&g_ep=EgoyMDI2MDIwNC4wIKXMDSoASAFQAw%3D%3D)

---

## Tuesday, Feb 10

**Booked**

- Check in @ 3pm [Windsor Plaza Copacabana](https://www.google.com/maps/place/Windsor+Plaza+Copacabana/@-22.9627805,-43.1776329,17z/data=!4m9!3m8!1s0x997fff867cdf99:0x97e28a466acef850!5m2!4m1!1i2!8m2!3d-22.9627855!4d-43.175058!16s%2Fg%2F1tzl_5js?entry=ttu&g_ep=EgoyMDI2MDIwNC4wIKXMDSoASAFQAw%3D%3D)

_Optional_

---

## Wednesday, Feb 11

**Booked**

_Optional_

---

## Thursday, Feb 12

**Booked**

_Optional_

---

## Friday, Feb 13

**Booked**

- BOMA presents: Jamie Jones, Adam Ten, & Mita Gami
  - Time: 10pm
  - Address: [Museu do Amanhã, Rio de Janeiro - RJ](https://www.google.com/maps/place/Museu+do+Amanh%C3%A3,+Rio+de+Janeiro+-+RJ/data=!4m2!3m1!1s0x997f5732a12a31:0x316901f971660ce1?sa=X&ved=1t:155783&ictx=111)
  - App: Ingresse

_Optional_

---

## Saturday, Feb 14

**Booked**

- Rio Carnaval: Sambadrome parades
  - VIP access (Camarote MAR)
  - App: Quentro
  - Requires us to pick up shirts and verify facial recognition ahead of time
    - Kit pickup address: [Jockey Club Brasileiro – Tribuna Praça Santos Dumont, 31 – Gávea, Rio de Janeiro](https://www.google.com/maps/place/Jockey+Club+Brasileiro/@-22.9808712,-43.236486,14.64z/data=!4m6!3m5!1s0x9bd5a4a2deac4b:0xb3e7dcff9fe1c860!8m2!3d-22.9735236!4d-43.2247015!16s%2Fg%2F122c1d3s?entry=ttu&g_ep=EgoyMDI2MDIwNC4wIKXMDSoASAFQAw%3D%3D)
    - Kit pickup time: 2pm-10pm every day we're there
    - Source: [email](https://mail.google.com/mail/u/0/#search/rio/FMfcgzQfBkGjDRkvtsRtCsHRjWZnLCQV)
  - Event time: Opens at 7pm, Starts at 8pm
  - Event address: [Marquês de Sapucaí, Rio de Janeiro](https://www.google.com/maps/place/Sambadrome+Marqu%C3%AAs+de+Sapuca%C3%AD/@-22.9114472,-43.1993789,17z/data=!3m2!4b1!5s0x997f0d0c24ef39:0x3333a5703a71abdc!4m6!3m5!1s0x997f0d91d00145:0xd0816e87c570167d!8m2!3d-22.9114522!4d-43.196804!16zL20vMGJwd3Fw?entry=ttu&g_ep=EgoyMDI2MDIwNC4wIKXMDSoASAFQAw%3D%3D)

_Optional_

---

## Sunday, Feb 15

**Booked**

_Optional_

---

## Monday, Feb 16

**Booked**

_Optional_

---

## Tuesday, Feb 17

**Booked**

_Optional_

---

## Wednesday, Feb 18

**Booked**

_Optional_

---

## Thursday, February 19 — Departure

**Booked**

- Hotel check out @ 12pm
- Flight departs @ 9:30 PM

_Optional_

- Last-minute shopping
]]></content>
    </entry>
</feed>